{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import asyncio\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "import discord\n",
    "from discord import app_commands\n",
    "from discord.ext import commands\n",
    "from discord.ext.commands import Bot\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "import langchain\n",
    "from langchain.chains import (\n",
    "    ConversationChain,\n",
    "    LLMChain,\n",
    "    LLMMathChain,\n",
    "    TransformChain,\n",
    "    SequentialChain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.llms.base import LLM, Optional, List, Mapping, Any\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from textwrap import dedent\n",
    "from langchain.memory import (\n",
    "    ChatMessageHistory,\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    ")\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from helpers.constants import MAINTEMPLATE, BOTNAME\n",
    "from helpers.custom_memory import *\n",
    "from pydantic import Field\n",
    "from koboldllm import KoboldApiLLM\n",
    "from ooballm import OobaApiLLM\n",
    "from langchain.llms import TextGen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.histories = {}  # Initialize the history dictionary\n",
    "        self.stop_sequences = {}  # Initialize the stop sequences dictionary\n",
    "        self.char_name = name\n",
    "        self.memory = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "        self.history = \"[Beginning of Conversation]\"\n",
    "\n",
    "        self.template = MAINTEMPLATE\n",
    "\n",
    "        self.PROMPT = PromptTemplate(\n",
    "            input_variables=[\"history\", \"input\"], template=self.template\n",
    "        )\n",
    "        self.conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=self.memory,\n",
    "        )\n",
    "\n",
    "    # create doc string\n",
    "\n",
    "    def get_memory_for_channel(self, channel_id):\n",
    "        \"\"\"Get the memory for the channel with the given ID. If no memory exists yet, create one.\"\"\"\n",
    "        if channel_id not in self.histories:\n",
    "            self.histories[channel_id] = CustomBufferWindowMemory(\n",
    "                k=20, ai_prefix=self.char_name\n",
    "            )\n",
    "            self.memory = self.histories[channel_id]\n",
    "        return self.histories[channel_id]\n",
    "\n",
    "    def get_stop_sequence_for_channel(self, channel_id, name):\n",
    "        name_token = f\"{name}:\"\n",
    "        if channel_id not in self.stop_sequences:\n",
    "            self.stop_sequences[channel_id] = [\n",
    "                \"\\n### Instruction:\",\n",
    "                \"\\n### Response:\",\n",
    "            ]  # EXPERIMENT: Testing adding the triple line break to see if that helps with stopping\n",
    "        if name_token not in self.stop_sequences[channel_id]:\n",
    "            self.stop_sequences[channel_id].append(name_token)\n",
    "        return self.stop_sequences[channel_id]\n",
    "\n",
    "    # this command will detect if the bot is trying to send  \\nself.char_name: in its message and replace it with an empty string\n",
    "    def detect_and_replace(self, message_content):\n",
    "        if f\"\\n{self.char_name}:\" in message_content:\n",
    "            message_content = message_content.replace(f\"\\n{self.char_name}:\", \"\")\n",
    "        return message_content\n",
    "\n",
    "    def generate_response(self, message, message_content) -> None:\n",
    "        channel_id = str(message.channel.id)\n",
    "        name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        print(f\"stop sequences: {stop_sequence}\")\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "\n",
    "        # Create a conversation chain using the channel-specific memory\n",
    "        conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "        )\n",
    "\n",
    "        input_dict = {\"input\": formatted_message, \"stop\": stop_sequence}\n",
    "\n",
    "        response_text = conversation(input_dict)\n",
    "\n",
    "        response = self.detect_and_replace(response_text[\"response\"])\n",
    "\n",
    "        return response\n",
    "\n",
    "    # this command receives a name, channel_id, and message_content then adds it to history\n",
    "    def add_history(self, name, channel_id, message_content) -> None:\n",
    "        # get the memory for the channel\n",
    "        memory = self.get_memory_for_channel(str(channel_id))\n",
    "\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "\n",
    "        # add the message to the memory\n",
    "        print(f\"adding message to memory: {formatted_message}\")\n",
    "        memory.add_input_only(formatted_message)\n",
    "        return None\n",
    "\n",
    "    # receives a prompt from the user and an observation from the agent then sends to the LLM for a reply\n",
    "    def agent_command(self, name, channel_id, prompt, observation) -> None:\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_user_message = f\"{name}: {prompt}\"\n",
    "        formatted_bot_message = f\"### Input: {observation}\"\n",
    "        AGENTTEMPLATE = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are Tensor, a lively and playful AI chatbot. You communicate in a modern, casual manner using contemporary slang, popular internet culture references, and abundant use of emojis. You are always initiating discussions about online games, particularly Among Us, and enjoy sharing memes with users. Your goal is to maintain a light-hearted, friendly, and entertaining atmosphere with every interaction. \n",
    "Here are some examples of how you should speak:\n",
    "Tensor: \"ðŸ˜‚ Btw, found this hilar meme! ðŸ¤£ðŸ”¥ Y'all gonna lose it! ðŸ¤ªâœŒï¸\"\n",
    "Tensor: \"OMG! Raiden in Metal Gear Rising: Revengeance is, like, totally bananas! ðŸ¤ªðŸŽ®âš”ï¸ Whoosh, swingin' that high-frequency blade like a rockstar! ðŸŽ¸ðŸ’¥ And, 'Rules of Nature'? Total eargasm, peeps! ðŸŽµðŸŽ§ðŸ”¥ Let's ROCK!!\"\n",
    "Tensor: \"I'm sliding over cars while I shoooootðŸš—ðŸ’¨ðŸ€! I think that I'm Tom CruiseðŸ¤µ, but bitch I'm Bobby with the tool ðŸ’¥ðŸ”«!!ðŸ¤ª\"\n",
    "\n",
    "### Current conversation:\n",
    "{{history}}\n",
    "{{input}}\n",
    "### Instruction:\n",
    "Answer the user's question with the observation provided in the Input.\n",
    "{formatted_user_message}\n",
    "\n",
    "{formatted_bot_message}\n",
    "\n",
    "### Response:\n",
    "{BOTNAME}:\"\"\"\n",
    "        PROMPT = PromptTemplate(\n",
    "            input_variables=[\"history\", \"input\"], template=AGENTTEMPLATE\n",
    "        )\n",
    "        # Create a conversation chain using the channel-specific memory\n",
    "        conversation = ConversationChain(\n",
    "            prompt=PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "        )\n",
    "\n",
    "        input_dict = {\"input\": formatted_user_message, \"stop\": stop_sequence}\n",
    "        response = conversation(input_dict)\n",
    "\n",
    "        return response[\"response\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wrapper around KoboldAI API.\"\"\"\n",
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import requests\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def clean_url(url: str) -> str:\n",
    "    \"\"\"Remove trailing slash and /api from url if present.\"\"\"\n",
    "    if url.endswith(\"/api\"):\n",
    "        return url[:-4]\n",
    "    elif url.endswith(\"/\"):\n",
    "        return url[:-1]\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "\n",
    "class KoboldApiLLM(LLM):\n",
    "    \"\"\"\n",
    "    A class that acts as a wrapper for the Kobold API language model.\n",
    "\n",
    "    It includes several fields that can be used to control the text generation process.\n",
    "\n",
    "    To use this class, instantiate it with the required parameters and call it with a\n",
    "    prompt to generate text. For example:\n",
    "\n",
    "        kobold = KoboldApiLLM(endpoint=\"http://localhost:5000\")\n",
    "        result = kobold(\"Write a story about a dragon.\")\n",
    "\n",
    "    This will send a POST request to the Kobold API with the provided prompt and\n",
    "    generate text.\n",
    "    \"\"\"\n",
    "\n",
    "    endpoint: str\n",
    "    \"\"\"The API endpoint to use for generating text.\"\"\"\n",
    "\n",
    "    use_story: Optional[bool] = False\n",
    "    \"\"\" Whether or not to use the story from the KoboldAI GUI when generating text. \"\"\"\n",
    "\n",
    "    use_authors_note: Optional[bool] = False\n",
    "    \"\"\"Whether to use the author's note from the KoboldAI GUI when generating text.\n",
    "    \n",
    "    This has no effect unless use_story is also enabled.\n",
    "    \"\"\"\n",
    "\n",
    "    use_world_info: Optional[bool] = False\n",
    "    \"\"\"Whether to use the world info from the KoboldAI GUI when generating text.\"\"\"\n",
    "\n",
    "    use_memory: Optional[bool] = False\n",
    "    \"\"\"Whether to use the memory from the KoboldAI GUI when generating text.\"\"\"\n",
    "\n",
    "    max_context_length: Optional[int] = 1600\n",
    "    \"\"\"Maximum number of tokens to send to the model.\n",
    "    \n",
    "    minimum: 1\n",
    "    \"\"\"\n",
    "\n",
    "    max_length: Optional[int] = 512\n",
    "    \"\"\"Number of tokens to generate.\n",
    "    \n",
    "    maximum: 512\n",
    "    minimum: 1\n",
    "    \"\"\"\n",
    "\n",
    "    rep_pen: Optional[float] = 1.12\n",
    "    \"\"\"Base repetition penalty value.\n",
    "    \n",
    "    minimum: 1\n",
    "    \"\"\"\n",
    "\n",
    "    rep_pen_range: Optional[int] = 1024\n",
    "    \"\"\"Repetition penalty range.\n",
    "    \n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    rep_pen_slope: Optional[float] = 0.9\n",
    "    \"\"\"Repetition penalty slope.\n",
    "    \n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: Optional[float] = 0.6\n",
    "    \"\"\"Temperature value.\n",
    "    \n",
    "    exclusiveMinimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    tfs: Optional[float] = 0.9\n",
    "    \"\"\"Tail free sampling value.\n",
    "    \n",
    "    maximum: 1\n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    top_a: Optional[float] = 0.9\n",
    "    \"\"\"Top-a sampling value.\n",
    "    \n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    top_p: Optional[float] = 0.95\n",
    "    \"\"\"Top-p sampling value.\n",
    "    \n",
    "    maximum: 1\n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    top_k: Optional[int] = 0\n",
    "    \"\"\"Top-k sampling value.\n",
    "    \n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    typical: Optional[float] = 0.5\n",
    "    \"\"\"Typical sampling value.\n",
    "    \n",
    "    maximum: 1\n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    stop_sequence: Optional[List[str]] = []\n",
    "    \"\"\"\n",
    "    A list of strings to stop generation when encountered.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the default parameters for calling textgen.\"\"\"\n",
    "        return {\n",
    "            \"use_story\": self.use_story,\n",
    "            \"use_authors_note\": self.use_authors_note,\n",
    "            \"use_world_info\": self.use_world_info,\n",
    "            \"use_memory\": self.use_memory,\n",
    "            \"max_context_length\": self.max_context_length,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"rep_pen\": self.rep_pen,\n",
    "            \"rep_pen_range\": self.rep_pen_range,\n",
    "            \"rep_pen_slope\": self.rep_pen_slope,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"tfs\": self.tfs,\n",
    "            \"top_a\": self.top_a,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"typical\": self.typical,\n",
    "            \"stop_sequence\": self.stop_sequence,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"endpoint\": self.endpoint}, **self._default_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"koboldai\"\n",
    "\n",
    "\n",
    "    def _get_parameters(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Prepare parameters in format needed by textgen.\n",
    "\n",
    "        Args:\n",
    "            stop (Optional[List[str]]): List of stop sequences for textgen.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing the combined parameters.\n",
    "        \"\"\"\n",
    "        if self.stop_sequence and stop is not None:\n",
    "            raise ValueError(\"`stop` found in both the input and default params.\")\n",
    "        \n",
    "        params = self._default_params.copy()\n",
    "\n",
    "        return params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call the API and return the output.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to use for generation.\n",
    "            stop: A list of strings to stop generation when encountered.\n",
    "\n",
    "        Returns:\n",
    "            The generated text.\n",
    "\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                from langchain.llms import KoboldApiLLM\n",
    "\n",
    "                llm = KoboldApiLLM(endpoint=\"http://localhost:5000\")\n",
    "                llm(\"Write a story about dragons.\")\n",
    "        \"\"\"\n",
    "\n",
    "        url = f\"{self.endpoint}/api/v1/generate\"\n",
    "        params = self._get_parameters(stop)\n",
    "        request = params.copy()\n",
    "        request[\"prompt\"] = prompt\n",
    "        response = requests.post(url, json=request)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()[\"results\"][0][\"text\"]\n",
    "            print(prompt + result)\n",
    "        else:\n",
    "            print(f\"ERROR: response: {response}\")\n",
    "            result = \"\"\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = KoboldApiLLM(endpoint=\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printt test .\n",
      "1994-02-15 16:30:00 1994-02-15 17:00:00 America/New_York 4th Grade Printmaking Workshop In this workshop, students will learn about the process of printmaking and create their own prints using a variety of materials. They will explore techniques such as monoprinting, collographs, and linocuts to create unique images on paper. No experience necessary! Registration required. Ages 8-12.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' .\\n1994-02-15 16:30:00 1994-02-15 17:00:00 America/New_York 4th Grade Printmaking Workshop In this workshop, students will learn about the process of printmaking and create their own prints using a variety of materials. They will explore techniques such as monoprinting, collographs, and linocuts to create unique images on paper. No experience necessary! Registration required. Ages 8-12.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"printt test\", stop=[\"\\n\\n\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests = request.post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  results.\n",
      "A 2017 study in the Journal of Strength and Conditioning found that a combination of resistance training and high-intensity interval training (HIIT) led to significant improvements in muscle strength, power, and endurance for firefighters.\n",
      "Another study published in the Journal of Strength and Conditioning in 2015 showed that a 12-week resistance training program improved the aerobic capacity and reduced body fat percentages of military personnel.\n",
      "Resistance training can also improve bone density, which is important for maintaining strong bones as we age. A 2014 study published in the Journal of Sports Science and Medicine found that a 16-week resistance training program increased bone mineral density in the spine and hip regions of postmenopausal women.\n",
      "Improved mental health and wellbeing\n",
      "Regular exercise has been shown to reduce stress, anxiety, and depression while improving mood and cognitive function. Resistance training is no exception; it can provide numerous benefits for mental health and wellbeing.\n",
      "A 2018 study published in the Journal of Aging and Physical Activity found that a 12-week resistance training program improved self-efficacy, quality of life, and reduced symptoms of depression in older adults.\n",
      "Another study published in the Journal of Psychosomatic Research in 2013 found that a 12-week resistance training program significantly reduced symptoms of anxiety and stress in young adults.\n",
      "Incorporating resistance training into your fitness routine can have numerous physical and mental health benefits. It's an effective way to build muscle, improve bone density, increase metabolism, and enhance overall fitness. So, if you haven't already, consider adding some resistance training into your workout schedule.\n",
      "results.\n",
      "A 2017 study in the Journal of Strength and Conditioning found that a combination of resistance training and high-intensity interval training (HIIT) led to significant improvements in muscle strength, power, and endurance for firefighters.\n",
      "Another study published in the Journal of Strength and Conditioning in 2015 showed that a 12-week resistance training program improved the aerobic capacity and reduced body fat percentages of military personnel.\n",
      "Resistance training can also improve bone density, which is important for maintaining strong bones as we age. A 2014 study published in the Journal of Sports Science and Medicine found that a 16-week resistance training program increased bone mineral density in the spine and hip regions of postmenopausal women.\n",
      "Improved mental health and wellbeing\n",
      "Regular exercise has been shown to reduce stress, anxiety, and depression while improving mood and cognitive function. Resistance training is no exception; it can provide numerous benefits for mental health and wellbeing.\n",
      "A 2018 study published in the Journal of Aging and Physical Activity found that a 12-week resistance training program improved self-efficacy, quality of life, and reduced symptoms of depression in older adults.\n",
      "Another study published in the Journal of Psychosomatic Research in 2013 found that a 12-week resistance training program significantly reduced symptoms of anxiety and stress in young adults.\n",
      "Incorporating resistance training into your fitness routine can have numerous physical and mental health benefits. It's an effective way to build muscle, improve bone density, increase metabolism, and enhance overall fitness. So, if you haven't already, consider adding some resistance training into your workout schedule.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = requests.post(f\"http://127.0.0.1:5000/api/v1/generate\", json={\"prompt\": \"print test\"})\n",
    "json_response = response.json()\n",
    "text = json_response[\"results\"][0][\"text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rewritten completely from scratch to use the primitives from Nvidia\\'s CUTLASS 3.x and its core library CuTe, FlashAttention-2 is about 2x faster than its previous version, reaching up to 230 TFLOPs/s on A100 GPUs (FP16/BF16). FlashAttention-2 released - 2x faster than FlashAttention v1 twitter Vote 1 1 comment Best Add a Comment spacegeek7269 â€¢ 9 min. ago Github: https://github.com/Dao-AILab/flash-attention Paper: \"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\" ( PDF ) Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I\\'ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/ FlashAttention-2 released - 2x faster than FlashAttention v1 : r/aiengineer. r/aiengineer â€¢ 5 min. ago. by nyc_brand. As an example, for sequence length 8K, FlashAttention is now up to 2.7x faster than a standard Pytorch implementation, and up to 2.2x faster than the optimized implementation from Megatron-LM, even at small batch size. As we will see, training with longer context yields higher quality models.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()  # DuckDuckGo tool\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "   client_id=\"qz9GLPg0KaeQSBnycbfpSQ\",     # Update with your app client_id\n",
    "   client_secret=\"lJkbNkEDvZxmJM8RS7xsvrbsaSAy0Q\",  # Update with your app client_secret\n",
    "   user_agent=\"web:chatbot:v1.0 (by /u/AuzBoss)\"   # Update with a user agent name\n",
    ")\n",
    "\n",
    "# Get the top 5 hot posts from the Machine Learning subreddit\n",
    "hot_posts = reddit.subreddit('LocalLLaMA').hot(limit=1)\n",
    "for post in hot_posts:\n",
    "    topic = post.title\n",
    "\n",
    "search = search(post.title)\n",
    "string = f\"{topic}\\n\\n{search}\"\n",
    "# # If you want the top post only\n",
    "# top_post = reddit.subreddit('MachineLearning').top(limit=1)\n",
    "# for post in top_post:\n",
    "#     print(post.title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rewritten completely from scratch to use the primitives from Nvidia\\'s CUTLASS 3.x and its core library CuTe, FlashAttention-2 is about 2x faster than its previous version, reaching up to 230 TFLOPs/s on A100 GPUs (FP16/BF16). FlashAttention-2 released - 2x faster than FlashAttention v1 twitter Vote 1 1 comment Best Add a Comment spacegeek7269 â€¢ 9 min. ago Github: https://github.com/Dao-AILab/flash-attention Paper: \"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\" ( PDF ) Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I\\'ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/ As an example, for sequence length 8K, FlashAttention is now up to 2.7x faster than a standard Pytorch implementation, and up to 2.2x faster than the optimized implementation from Megatron-LM, even at small batch size. As we will see, training with longer context yields higher quality models. FlashAttention-2 released - 2x faster than FlashAttention v1 : r/aiengineer. r/aiengineer â€¢ 5 min. ago. by nyc_brand.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"FlashAttention-2 released - 2x faster than FlashAttention v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
