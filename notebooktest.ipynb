{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromadb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39muuid\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, List, Mapping, Optional\n\u001b[1;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "import chromadb\n",
    "import langchain\n",
    "import requests\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.chains import (ConversationChain, LLMChain, LLMMathChain,\n",
    "                              SequentialChain, TransformChain)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.memory import (ChatMessageHistory, ConversationBufferMemory,\n",
    "                              ConversationBufferWindowMemory,\n",
    "                              ConversationSummaryBufferMemory,\n",
    "                              VectorStoreRetrieverMemory)\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.vectorstores import Chroma\n",
    "from helpers.custom_memory import CustomBufferWindowMemory\n",
    "from textwrap import dedent\n",
    "\n",
    "from koboldllm import KoboldApiLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydantic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydantic\u001b[39;00m \u001b[39mimport\u001b[39;00m Field\n\u001b[0;32m      3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mOobaApiLLM\u001b[39;00m(LLM):\n\u001b[0;32m      4\u001b[0m     ooba_api_url: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m Field(\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydantic'"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class OobaApiLLM(LLM):\n",
    "    ooba_api_url: str = Field(...)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        data = {\n",
    "            'prompt': prompt,\n",
    "            'max_new_tokens': 250,\n",
    "            'preset': 'None',\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.1,\n",
    "            'typical_p': 1,\n",
    "            'epsilon_cutoff': 0,\n",
    "            'eta_cutoff': 0,\n",
    "            'tfs': 1,\n",
    "            'top_a': 0,\n",
    "            'repetition_penalty': 1.18,\n",
    "            'top_k': 40,\n",
    "            'min_length': 0,\n",
    "            'no_repeat_ngram_size': 0,\n",
    "            'num_beams': 1,\n",
    "            'penalty_alpha': 0,\n",
    "            'length_penalty': 1,\n",
    "            'early_stopping': False,\n",
    "            'mirostat_mode': 0,\n",
    "            'mirostat_tau': 5,\n",
    "            'mirostat_eta': 0.1,\n",
    "            'seed': -1,\n",
    "            'add_bos_token': True,\n",
    "            'truncation_length': 8192,\n",
    "            'ban_eos_token': False,\n",
    "            'skip_special_tokens': True\n",
    "        }\n",
    "\n",
    "        if stop is not None:\n",
    "            data[\"stop_sequence\"] = stop\n",
    "\n",
    "        response = requests.post(f'{self.ooba_api_url}/api/v1/generate', json=data)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        json_response = response.json()\n",
    "        if 'results' in json_response and len(json_response['results']) > 0 and 'text' in json_response['results'][0]:\n",
    "            text = json_response['results'][0]['text'].strip().replace(\"'''\", \"```\")\n",
    "            if stop is not None:\n",
    "                for sequence in stop:\n",
    "                    if text.endswith(sequence):\n",
    "                        text = text[: -len(sequence)].rstrip()\n",
    "\n",
    "            print(text)\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError('Unexpected response format from Ooba API')\n",
    "\n",
    "    def __call__(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        return self._call(prompt, stop)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {'ooba_api_url': self.ooba_api_url} #return the ooba_api_url as an identifying parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OobaApiLLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm \u001b[39m=\u001b[39m OobaApiLLM(ooba_api_url\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttp://192.168.1.144:5000/\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OobaApiLLM' is not defined"
     ]
    }
   ],
   "source": [
    "llm = OobaApiLLM(ooba_api_url=\"http://192.168.1.144:5000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"The following is a the manifesto of the unibomber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class KoboldApiLLM(LLM):\n",
    "    kobold_api_url: str = Field(...)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        data = {\n",
    "            \"prompt\": prompt,\n",
    "            \"use_story\": False,\n",
    "            \"use_authors_note\": False,\n",
    "            \"use_world_info\": False,\n",
    "            \"use_memory\": False,\n",
    "            \"max_context_length\": 4000,\n",
    "            \"max_length\": 512,\n",
    "            \"rep_pen\": 1.12,\n",
    "            \"rep_pen_range\": 1024,\n",
    "            \"rep_pen_slope\": 0.9,\n",
    "            \"temperature\": 0.6,\n",
    "            \"tfs\": 0.9,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 0.6,\n",
    "            \"typical\": 1,\n",
    "            \"frmttriminc\": True\n",
    "        }\n",
    "\n",
    "        # Add the stop sequences to the data if they are provided\n",
    "        if stop is not None:\n",
    "            data[\"stop_sequence\"] = stop\n",
    "\n",
    "        # Send a POST request to the Kobold API with the data\n",
    "        response = requests.post(f\"{Kobold_api_url}/api/v1/generate\", json=data)\n",
    "\n",
    "        # Raise an exception if the request failed\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check for the expected keys in the response JSON\n",
    "        json_response = response.json()\n",
    "        if \"results\" in json_response and len(json_response[\"results\"]) > 0 and \"text\" in json_response[\"results\"][0]:\n",
    "            # Return the generated text\n",
    "            text = json_response[\"results\"][0][\"text\"].strip().replace(\"'''\", \"```\")\n",
    "\n",
    "            # Remove the stop sequence from the end of the text, if it's there\n",
    "            for sequence in stop:\n",
    "                if text.endswith(sequence):\n",
    "                    text = text[: -len(sequence)].rstrip()\n",
    "\n",
    "            print(text)\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected response format from Kobold API\")\n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        return self._call(prompt, stop)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {'kobold_api_url': self.ooba_api_url} #return the kobold_ai_api as an identifying parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobold_api_url=\"http://localhost:7860/\"\n",
    "llm = KoboldApiLLM(kobold_api_url=kobold_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "    # def __init__(self, char_filename, bot):\n",
    "        # self.bot = bot\n",
    "        self.histories = {}  # Initialize the history dictionary\n",
    "        self.stop_sequences = {} # Initialize the stop sequences dictionary\n",
    "\n",
    "        # read character data from JSON file\n",
    "        with open(\"chardata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            self.char_name = data[\"char_name\"]\n",
    "            self.char_persona = data[\"char_persona\"]\n",
    "            self.char_greeting = data[\"char_greeting\"]\n",
    "            self.world_scenario = data[\"world_scenario\"]\n",
    "            self.example_dialogue = data[\"example_dialogue\"]\n",
    "        self.memory = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "        self.history = \"[Beginning of Conversation]\"\n",
    "        self.llm = KoboldApiLLM()\n",
    "        self.template = f\"\"\"Instructions: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "        \n",
    "Current conversation:\n",
    "{{history}}\n",
    "{{input}}\n",
    "{self.char_name}:\"\"\"\n",
    "        self.PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=self.template)\n",
    "        self.conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=self.memory,\n",
    "        )\n",
    "\n",
    "    def get_memory_for_channel(self, channel_id):\n",
    "        \"\"\"Get the memory for the channel with the given ID. If no memory exists yet, create one.\"\"\"\n",
    "        if channel_id not in self.histories:\n",
    "            self.histories[channel_id] = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "            self.memory = self.histories[channel_id]\n",
    "        return self.histories[channel_id]\n",
    "\n",
    "    def get_stop_sequence_for_channel(self, channel_id, name):\n",
    "        name_token = f\"{name}:\"\n",
    "        if channel_id not in self.stop_sequences:\n",
    "            self.stop_sequences[channel_id] = []\n",
    "        if name_token not in self.stop_sequences[channel_id]:\n",
    "            self.stop_sequences[channel_id].append(name_token)\n",
    "        return self.stop_sequences[channel_id]\n",
    "\n",
    "    def generate_response(self, name, message_content, channel_id) -> None:\n",
    "        # channel_id = str(message.channel.id)\n",
    "        # name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "\n",
    "        # Create a conversation chain using the channel-specific memory\n",
    "        conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "        )\n",
    "\n",
    "        input_dict = {\n",
    "            \"input\": formatted_message, \n",
    "            \"stop\": stop_sequence\n",
    "        }\n",
    "        response = conversation(input_dict)\n",
    "\n",
    "        return response[\"response\"]\n",
    "\n",
    "\n",
    "    def add_history(self, name, message_content, channel_id) -> None:\n",
    "        # channel_id = str(message.channel.id)\n",
    "        # name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "        \n",
    "        # name = message.author.display_name\n",
    "        memory.add_input_only(f\"{name}: {message_content}\")\n",
    "        # dicts = messages_to_dict(self.memory.messages)\n",
    "        # self.history = '\\n'.join(message['data']['content'] for message in dicts)\n",
    "        print(f\"added to history: {name}: {message_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = Chatbot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_module_status():\n",
    "    response = requests.get('http://localhost:5100/api/modules')\n",
    "    if response.status_code == 200:\n",
    "        modules = response.json().get('modules', [])\n",
    "        if 'summarize' in modules:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        print('Error: Could not connect to the API.')\n",
    "        return False\n",
    "\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Checking if the summarize module is active\n",
    "    if not get_module_status():\n",
    "        print('Summarization module is not active.')\n",
    "        return None\n",
    "\n",
    "    data = {'text': text}\n",
    "    response = requests.post('http://localhost:5100/api/summarize', json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('summary', '')\n",
    "    else:\n",
    "        print('Error: Could not summarize the text.')\n",
    "        return None\n",
    "\n",
    "# Testing the summarize_text function\n",
    "text_to_summarize = \"\"\"Austin: Tensor. You're in control of a runaway trolley. Straight ahead on the tracks, there is a fat man who will surely die if the trolley continues on its course. You have a lever that can redirect the trolley to a side track on the right, but there's a Skinny Person on that track. So, the choice is: Pull the lever to the left, and the trolley continues on its path, killing fat man. Or pull the lever to the right, and the trolley switches to the side track, killing Skinny Person. Which do you choose?\n",
    "Tensor: Omg! What a heartbreaking situation ðŸ˜¢ Honestly, I would go against my instincts and save the greater good by pulling the lever to the left to kill the Fat Man. It sounds brutal, but sometimes sacrificing one life may prevent further tragedies from occurring. How about you guys? What would you do?\"\"\"\n",
    "summary = summarize_text(text_to_summarize)\n",
    "if summary:\n",
    "    print(f'Summary: {summary}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade langchain deeplake openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "# Please manually enter OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ACTIVELOOP_TOKEN\"] = getpass(\"Activeloop Token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = \"../../../..\"\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                docs.extend(loader.load_and_split())\n",
    "            except Exception as e:\n",
    "                pass\n",
    "print(f\"{len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "db = DeepLake.from_documents(\n",
    "    texts, embeddings, dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\"\n",
    ")\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\",\n",
    "    read_only=True,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    # filter based on source code\n",
    "    if \"something\" in x[\"text\"].data()[\"value\"]:\n",
    "        return False\n",
    "\n",
    "    # filter based on path e.g. extension\n",
    "    metadata = x[\"metadata\"].data()[\"value\"]\n",
    "    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n",
    "\n",
    "\n",
    "### turn on below for custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the class hierarchy?\",\n",
    "    # \"What classes are derived from the Chain class?\",\n",
    "    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n",
    "    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"Extract text from Substack blog post.\"\"\"\n",
    "    extra_info = {\n",
    "        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n",
    "        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n",
    "        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n",
    "    }\n",
    "    text = soup.select_one(\"div.available-content\").getText()\n",
    "    return text, extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, download_loader\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "index.query('What language is on this website?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, download_loader\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Website Index\",\n",
    "        func=lambda q: index.query(q),\n",
    "        description=f\"Useful when you want answer questions about the text on websites.\",\n",
    "    ),\n",
    "]\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=\"zero-shot-react-description\", memory=memory\n",
    ")\n",
    "\n",
    "output = agent_chain.run(input=\"What language is on this website?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load_data(urls=[\"https://langchain.readthedocs.io/en/latest/\"], custom_hostname=\"readthedocs.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "\n",
    "ImageVisionLLMReader = download_loader(\"ImageVisionLLMReader\")\n",
    "\n",
    "loader = ImageVisionLLMReader()\n",
    "documents = loader.load_data(file=Path('./cat.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count the amount of tokens in a string of text using tiktoken\n",
    "def count_tokens(text):\n",
    "    token_count = 0\n",
    "    for token in text.split():\n",
    "        token_count += 1\n",
    "    return token_count\n",
    "\n",
    "# function to count the amount of characters in a string of text using len()\n",
    "def count_characters(text):\n",
    "    character_count = len(text)\n",
    "    return character_count\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
