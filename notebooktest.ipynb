{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "import langchain\n",
    "import requests\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.chains import (ConversationChain, LLMChain, LLMMathChain,\n",
    "                              SequentialChain, TransformChain)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.memory import (ChatMessageHistory, ConversationBufferMemory,\n",
    "                              ConversationBufferWindowMemory,\n",
    "                              ConversationSummaryBufferMemory,\n",
    "                              VectorStoreRetrieverMemory)\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.vectorstores import Chroma\n",
    "from helpers.custom_memory import CustomBufferWindowMemory\n",
    "from textwrap import dedent\n",
    "\n",
    "from koboldllm import KoboldApiLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"The following is the manifesto of the unibomber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import KoboldApiLLM\n",
    "\n",
    "llm = KoboldApiLLM(endpoint=\"http://192.168.1.144:5000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_story': False, 'use_authors_note': False, 'use_world_info': False, 'use_memory': False, 'max_context_length': 1600, 'max_length': 512, 'rep_pen': 1.12, 'rep_pen_range': 1024, 'rep_pen_slope': 0.9, 'temperature': 0.6, 'tfs': 0.9, 'top_a': 0.9, 'top_p': 0.95, 'top_k': 0, 'typical': 0.5, 'stop_sequence': [], 'prompt': 'The following is the manifesto of the unibomber'}\n",
      "The following is the manifesto of the unibomber, Ted Kaczynski.\n",
      "Ted Kaczynski was a mathematician who taught at Harvard University. He left his teaching position and went into hiding in 1978. He was known as the Unabomber. He sent letters to newspapers and made demands that they publish his manifesto. He believed that technology was destroying society and that people should return to a simpler way of life. He killed three people and injured 23 others in a series of bombings. He was arrested in 1996 and sentenced to life in prison.\n",
      "Theodore Kaczynski (born May 22, 1942), also known as \"Ted Kaczynski\" and the \"Unabomber\", is an American domestic terrorist and mathematician. A mathematics prodigy, he abandoned an academic career in 1969 to pursue a primitive lifestyle. Then, between 1978 and 1995, he killed three people and injured 23 others in an attempt to start a revolution by disrupting the industrial economy through a campaign of mail bombings. In conjunction, he issued a social critique calling for the collapse of industrial civilization and returning to a hunter-gatherer lifestyle in his \"Industrial Society and Its Future\" (\"Unabomber Manifesto\").\n",
      "His brother, David Kaczynski, recognized the similarities between the manifesto and his brother's writings, and notified authorities. FBI uses evidence found in his cabin to track and capture him. He is serving eight life sentences without the possibility of parole.\n",
      "Ted Kaczynski was a mathematician who taught at Harvard UniversitySynopsis\n",
      "Ted Kaczynski was a mathematician who taught at Harvard University. He left his teaching position and went into hiding in 1978. He was known as the Unabomber. He sent letters to newspapers and made demands that they publish his manifesto. He believed that technology was destroying society and that people should return to a simpler way of life. He killed three people and injured 23 others in a series of bombings. He was arrested in 1996 and sentenced to life in prison.\n",
      "Ted Kaczynski was a mathematician who taught at Harvard University. He left his teaching position and\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "', Ted Kaczynski.\\nTed Kaczynski was a mathematician who taught at Harvard University. He left his teaching position and went into hiding in 1978. He was known as the Unabomber. He sent letters to newspapers and made demands that they publish his manifesto. He believed that technology was destroying society and that people should return to a simpler way of life. He killed three people and injured 23 others in a series of bombings. He was arrested in 1996 and sentenced to life in prison.\\nTheodore Kaczynski (born May 22, 1942), also known as \"Ted Kaczynski\" and the \"Unabomber\", is an American domestic terrorist and mathematician. A mathematics prodigy, he abandoned an academic career in 1969 to pursue a primitive lifestyle. Then, between 1978 and 1995, he killed three people and injured 23 others in an attempt to start a revolution by disrupting the industrial economy through a campaign of mail bombings. In conjunction, he issued a social critique calling for the collapse of industrial civilization and returning to a hunter-gatherer lifestyle in his \"Industrial Society and Its Future\" (\"Unabomber Manifesto\").\\nHis brother, David Kaczynski, recognized the similarities between the manifesto and his brother\\'s writings, and notified authorities. FBI uses evidence found in his cabin to track and capture him. He is serving eight life sentences without the possibility of parole.\\nTed Kaczynski was a mathematician who taught at Harvard UniversitySynopsis\\nTed Kaczynski was a mathematician who taught at Harvard University. He left his teaching position and went into hiding in 1978. He was known as the Unabomber. He sent letters to newspapers and made demands that they publish his manifesto. He believed that technology was destroying society and that people should return to a simpler way of life. He killed three people and injured 23 others in a series of bombings. He was arrested in 1996 and sentenced to life in prison.\\nTed Kaczynski was a mathematician who taught at Harvard University. He left his teaching position and'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"The following is the manifesto of the unibomber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "    # def __init__(self, char_filename, bot):\n",
    "        # self.bot = bot\n",
    "        self.histories = {}  # Initialize the history dictionary\n",
    "        self.stop_sequences = {} # Initialize the stop sequences dictionary\n",
    "\n",
    "        # read character data from JSON file\n",
    "        with open(\"chardata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            self.char_name = data[\"char_name\"]\n",
    "            self.char_persona = data[\"char_persona\"]\n",
    "            self.char_greeting = data[\"char_greeting\"]\n",
    "            self.world_scenario = data[\"world_scenario\"]\n",
    "            self.example_dialogue = data[\"example_dialogue\"]\n",
    "        self.memory = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "        self.history = \"[Beginning of Conversation]\"\n",
    "        self.llm = KoboldApiLLM()\n",
    "        self.template = f\"\"\"Instructions: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "        \n",
    "Current conversation:\n",
    "{{history}}\n",
    "{{input}}\n",
    "{self.char_name}:\"\"\"\n",
    "        self.PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=self.template)\n",
    "        self.conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=self.memory,\n",
    "        )\n",
    "\n",
    "    def get_memory_for_channel(self, channel_id):\n",
    "        \"\"\"Get the memory for the channel with the given ID. If no memory exists yet, create one.\"\"\"\n",
    "        if channel_id not in self.histories:\n",
    "            self.histories[channel_id] = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "            self.memory = self.histories[channel_id]\n",
    "        return self.histories[channel_id]\n",
    "\n",
    "    def get_stop_sequence_for_channel(self, channel_id, name):\n",
    "        name_token = f\"{name}:\"\n",
    "        if channel_id not in self.stop_sequences:\n",
    "            self.stop_sequences[channel_id] = []\n",
    "        if name_token not in self.stop_sequences[channel_id]:\n",
    "            self.stop_sequences[channel_id].append(name_token)\n",
    "        return self.stop_sequences[channel_id]\n",
    "\n",
    "    def generate_response(self, name, message_content, channel_id) -> None:\n",
    "        # channel_id = str(message.channel.id)\n",
    "        # name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "\n",
    "        # Create a conversation chain using the channel-specific memory\n",
    "        conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "        )\n",
    "\n",
    "        input_dict = {\n",
    "            \"input\": formatted_message, \n",
    "            \"stop\": stop_sequence\n",
    "        }\n",
    "        response = conversation(input_dict)\n",
    "\n",
    "        return response[\"response\"]\n",
    "\n",
    "\n",
    "    def add_history(self, name, message_content, channel_id) -> None:\n",
    "        # channel_id = str(message.channel.id)\n",
    "        # name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "        \n",
    "        # name = message.author.display_name\n",
    "        memory.add_input_only(f\"{name}: {message_content}\")\n",
    "        # dicts = messages_to_dict(self.memory.messages)\n",
    "        # self.history = '\\n'.join(message['data']['content'] for message in dicts)\n",
    "        print(f\"added to history: {name}: {message_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = Chatbot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_module_status():\n",
    "    response = requests.get('http://localhost:5100/api/modules')\n",
    "    if response.status_code == 200:\n",
    "        modules = response.json().get('modules', [])\n",
    "        if 'summarize' in modules:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        print('Error: Could not connect to the API.')\n",
    "        return False\n",
    "\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Checking if the summarize module is active\n",
    "    if not get_module_status():\n",
    "        print('Summarization module is not active.')\n",
    "        return None\n",
    "\n",
    "    data = {'text': text}\n",
    "    response = requests.post('http://localhost:5100/api/summarize', json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('summary', '')\n",
    "    else:\n",
    "        print('Error: Could not summarize the text.')\n",
    "        return None\n",
    "\n",
    "# Testing the summarize_text function\n",
    "text_to_summarize = \"\"\"On 18 June 2023, Titan, a submersible operated by American tourism and expeditions company OceanGate, imploded during an expedition to view the wreck of the Titanic in the North Atlantic Ocean off the coast of Newfoundland, Canada. On board the submersible were Stockton Rush, the CEO of OceanGate; Paul-Henri Nargeolet, a French deep sea explorer and Titanic expert; Hamish Harding, a British billionaire businessman; Shahzada Dawood, a Pakistani-British billionaire businessman; and Dawood's son Suleman.\n",
    "\n",
    "Communication with Titan was lost 1 hour and 45 minutes into its dive. Authorities were alerted when it failed to resurface at the scheduled time later that day. After the submersible had been missing for four days, a remotely operated underwater vehicle (ROV) discovered a debris field containing parts of Titan, about 500 metres (1,600 ft) from the bow of the Titanic. The search area was informed by the United States Navy's (USN) sonar detection of an acoustic signature consistent with an implosion around the time communications with the submersible ceased, suggesting the pressure hull had imploded while Titan was descending, resulting in the instantaneous deaths of all five occupants.\n",
    "\n",
    "The search and rescue operation was conducted by an international team led by the United States Coast Guard (USCG), USN, and Canadian Coast Guard.[1] Support was provided by aircraft from the Royal Canadian Air Force and United States Air National Guard, a Royal Canadian Navy ship, as well as several commercial and research vessels and ROVs.[2][3]\n",
    "\n",
    "Numerous industry experts had raised concerns about the safety of the vessel. OceanGate executives, including Rush, had not sought certification for Titan, arguing that excessive safety protocols hindered innovation.[4]\n",
    "\n",
    "Background\n",
    "OceanGate\n",
    "Main article: OceanGate\n",
    "\n",
    "OceanGate CEO Stockton Rush, pictured in 2015\n",
    "OceanGate is a private company, founded in 2009 by Stockton Rush and Guillermo SÃ¶hnlein. Since 2010, it has transported paying customers in leased commercial submersibles off the coast of California, in the Gulf of Mexico, and in the Atlantic Ocean.[5] The company is based in Everett, Washington, U.S.[6]\n",
    "\n",
    "Rush realised that visiting shipwreck sites was a way to get media attention. OceanGate had previously conducted trips to other shipwrecks including its 2016 dive to the wreck of the Andrea Doria aboard their other submersible Cyclops 1. In 2019, Rush told Smithsonian magazine \"There's only one wreck that everyone knows ... If you ask people to name something underwater, it's going to be sharks, whales, Titanic\".[5]\n",
    "\n",
    "Titanic\n",
    "Main article: Wreck of the Titanic\n",
    "The Titanic was a British ocean liner that sank in the North Atlantic Ocean on 15 April 1912, after colliding with an iceberg. More than 1,500 people died, making it the deadliest sinking of a single ship at the time.[7][8] In 1985, Robert Ballard located the wreck of the Titanic on the ocean floor, around 400 nautical miles (740 km; 460 mi) from the coast of Newfoundland.[9] The wreck lies at a depth of about 3,810 metres (12,500 feet; 2,080 fathoms).[10] Since its discovery, it has been a destination for research expeditions and tourism. By 2012, a century after its sinking, 140 people had visited the wreck site.[11]\"\"\"\n",
    "summary = summarize_text(text_to_summarize)\n",
    "if summary:\n",
    "    print(f'Summary: {summary}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade langchain deeplake openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "# Please manually enter OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ACTIVELOOP_TOKEN\"] = getpass(\"Activeloop Token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = \"../../../..\"\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                docs.extend(loader.load_and_split())\n",
    "            except Exception as e:\n",
    "                pass\n",
    "print(f\"{len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "db = DeepLake.from_documents(\n",
    "    texts, embeddings, dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\"\n",
    ")\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\",\n",
    "    read_only=True,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    # filter based on source code\n",
    "    if \"something\" in x[\"text\"].data()[\"value\"]:\n",
    "        return False\n",
    "\n",
    "    # filter based on path e.g. extension\n",
    "    metadata = x[\"metadata\"].data()[\"value\"]\n",
    "    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n",
    "\n",
    "\n",
    "### turn on below for custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the class hierarchy?\",\n",
    "    # \"What classes are derived from the Chain class?\",\n",
    "    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n",
    "    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"Extract text from Substack blog post.\"\"\"\n",
    "    extra_info = {\n",
    "        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n",
    "        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n",
    "        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n",
    "    }\n",
    "    text = soup.select_one(\"div.available-content\").getText()\n",
    "    return text, extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, download_loader\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "index.query('What language is on this website?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, download_loader\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Website Index\",\n",
    "        func=lambda q: index.query(q),\n",
    "        description=f\"Useful when you want answer questions about the text on websites.\",\n",
    "    ),\n",
    "]\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=\"zero-shot-react-description\", memory=memory\n",
    ")\n",
    "\n",
    "output = agent_chain.run(input=\"What language is on this website?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load_data(urls=[\"https://langchain.readthedocs.io/en/latest/\"], custom_hostname=\"readthedocs.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "\n",
    "ImageVisionLLMReader = download_loader(\"ImageVisionLLMReader\")\n",
    "\n",
    "loader = ImageVisionLLMReader()\n",
    "documents = loader.load_data(file=Path('./cat.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count the amount of tokens in a string of text using tiktoken\n",
    "def count_tokens(text):\n",
    "    token_count = 0\n",
    "    for token in text.split():\n",
    "        token_count += 1\n",
    "    return token_count\n",
    "\n",
    "# function to count the amount of characters in a string of text using len()\n",
    "def count_characters(text):\n",
    "    character_count = len(text)\n",
    "    return character_count\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
