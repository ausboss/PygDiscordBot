{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KoboldApiLLM' from 'koboldllm' (c:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\koboldllm.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhelpers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcustom_memory\u001b[39;00m \u001b[39mimport\u001b[39;00m CustomBufferWindowMemory\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtextwrap\u001b[39;00m \u001b[39mimport\u001b[39;00m dedent\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkoboldllm\u001b[39;00m \u001b[39mimport\u001b[39;00m KoboldApiLLM\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KoboldApiLLM' from 'koboldllm' (c:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\koboldllm.py)"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "import langchain\n",
    "import requests\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.chains import (ConversationChain, LLMChain, LLMMathChain,\n",
    "                              SequentialChain, TransformChain)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.memory import (ChatMessageHistory, ConversationBufferMemory,\n",
    "                              ConversationBufferWindowMemory,\n",
    "                              ConversationSummaryBufferMemory,\n",
    "                              VectorStoreRetrieverMemory)\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.vectorstores import Chroma\n",
    "from helpers.custom_memory import CustomBufferWindowMemory\n",
    "from textwrap import dedent\n",
    "\n",
    "from koboldllm import KoboldApiLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class OobaApiLLM(LLM):\n",
    "    ooba_api_url: str = Field(...)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        data = {\n",
    "            'prompt': prompt,\n",
    "            'max_new_tokens': 250,\n",
    "            'preset': 'None',\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.1,\n",
    "            'typical_p': 1,\n",
    "            'epsilon_cutoff': 0,\n",
    "            'eta_cutoff': 0,\n",
    "            'tfs': 1,\n",
    "            'top_a': 0,\n",
    "            'repetition_penalty': 1.18,\n",
    "            'top_k': 40,\n",
    "            'min_length': 0,\n",
    "            'no_repeat_ngram_size': 0,\n",
    "            'num_beams': 1,\n",
    "            'penalty_alpha': 0,\n",
    "            'length_penalty': 1,\n",
    "            'early_stopping': False,\n",
    "            'mirostat_mode': 0,\n",
    "            'mirostat_tau': 5,\n",
    "            'mirostat_eta': 0.1,\n",
    "            'seed': -1,\n",
    "            'add_bos_token': True,\n",
    "            'truncation_length': 8192,\n",
    "            'ban_eos_token': False,\n",
    "            'skip_special_tokens': True\n",
    "        }\n",
    "\n",
    "        if stop is not None:\n",
    "            data[\"stop_sequence\"] = stop\n",
    "\n",
    "        response = requests.post(f'{self.ooba_api_url}/api/v1/generate', json=data)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        json_response = response.json()\n",
    "        if 'results' in json_response and len(json_response['results']) > 0 and 'text' in json_response['results'][0]:\n",
    "            text = json_response['results'][0]['text'].strip().replace(\"'''\", \"```\")\n",
    "            if stop is not None:\n",
    "                for sequence in stop:\n",
    "                    if text.endswith(sequence):\n",
    "                        text = text[: -len(sequence)].rstrip()\n",
    "\n",
    "            print(text)\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError('Unexpected response format from Ooba API')\n",
    "\n",
    "    def __call__(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        return self._call(prompt, stop)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {'ooba_api_url': self.ooba_api_url} #return the ooba_api_url as an identifying parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (0.0.218)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (2.0.17)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (1.25.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (1.10.9)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (0.5.8)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.17 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (0.0.17)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import TextGen\n",
    "llm = TextGen(model_url=\"https://9e04c78610742a4f0e.gradio.live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: response: <Response [404]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"The following is the manifesto of the unibomber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class KoboldApiLLM(LLM):\n",
    "    kobold_api_url: str = Field(...)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        data = {\n",
    "            \"prompt\": prompt,\n",
    "            \"use_story\": False,\n",
    "            \"use_authors_note\": False,\n",
    "            \"use_world_info\": False,\n",
    "            \"use_memory\": False,\n",
    "            \"max_context_length\": 4000,\n",
    "            \"max_length\": 512,\n",
    "            \"rep_pen\": 1.12,\n",
    "            \"rep_pen_range\": 1024,\n",
    "            \"rep_pen_slope\": 0.9,\n",
    "            \"temperature\": 0.6,\n",
    "            \"tfs\": 0.9,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 0.6,\n",
    "            \"typical\": 1,\n",
    "            \"frmttriminc\": True\n",
    "        }\n",
    "\n",
    "        # Add the stop sequences to the data if they are provided\n",
    "        if stop is not None:\n",
    "            data[\"stop_sequence\"] = stop\n",
    "\n",
    "        # Send a POST request to the Kobold API with the data\n",
    "        response = requests.post(f\"{Kobold_api_url}/api/v1/generate\", json=data)\n",
    "\n",
    "        # Raise an exception if the request failed\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check for the expected keys in the response JSON\n",
    "        json_response = response.json()\n",
    "        if \"results\" in json_response and len(json_response[\"results\"]) > 0 and \"text\" in json_response[\"results\"][0]:\n",
    "            # Return the generated text\n",
    "            text = json_response[\"results\"][0][\"text\"].strip().replace(\"'''\", \"```\")\n",
    "\n",
    "            # Remove the stop sequence from the end of the text, if it's there\n",
    "            for sequence in stop:\n",
    "                if text.endswith(sequence):\n",
    "                    text = text[: -len(sequence)].rstrip()\n",
    "\n",
    "            print(text)\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected response format from Kobold API\")\n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        return self._call(prompt, stop)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {'kobold_api_url': self.ooba_api_url} #return the kobold_ai_api as an identifying parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobold_api_url=\"http://localhost:7860/\"\n",
    "llm = KoboldApiLLM(kobold_api_url=kobold_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "    # def __init__(self, char_filename, bot):\n",
    "        # self.bot = bot\n",
    "        self.histories = {}  # Initialize the history dictionary\n",
    "        self.stop_sequences = {} # Initialize the stop sequences dictionary\n",
    "\n",
    "        # read character data from JSON file\n",
    "        with open(\"chardata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            self.char_name = data[\"char_name\"]\n",
    "            self.char_persona = data[\"char_persona\"]\n",
    "            self.char_greeting = data[\"char_greeting\"]\n",
    "            self.world_scenario = data[\"world_scenario\"]\n",
    "            self.example_dialogue = data[\"example_dialogue\"]\n",
    "        self.memory = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "        self.history = \"[Beginning of Conversation]\"\n",
    "        self.llm = KoboldApiLLM()\n",
    "        self.template = f\"\"\"Instructions: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "        \n",
    "Current conversation:\n",
    "{{history}}\n",
    "{{input}}\n",
    "{self.char_name}:\"\"\"\n",
    "        self.PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=self.template)\n",
    "        self.conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=self.memory,\n",
    "        )\n",
    "\n",
    "    def get_memory_for_channel(self, channel_id):\n",
    "        \"\"\"Get the memory for the channel with the given ID. If no memory exists yet, create one.\"\"\"\n",
    "        if channel_id not in self.histories:\n",
    "            self.histories[channel_id] = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "            self.memory = self.histories[channel_id]\n",
    "        return self.histories[channel_id]\n",
    "\n",
    "    def get_stop_sequence_for_channel(self, channel_id, name):\n",
    "        name_token = f\"{name}:\"\n",
    "        if channel_id not in self.stop_sequences:\n",
    "            self.stop_sequences[channel_id] = []\n",
    "        if name_token not in self.stop_sequences[channel_id]:\n",
    "            self.stop_sequences[channel_id].append(name_token)\n",
    "        return self.stop_sequences[channel_id]\n",
    "\n",
    "    def generate_response(self, name, message_content, channel_id) -> None:\n",
    "        # channel_id = str(message.channel.id)\n",
    "        # name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "\n",
    "        # Create a conversation chain using the channel-specific memory\n",
    "        conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "        )\n",
    "\n",
    "        input_dict = {\n",
    "            \"input\": formatted_message, \n",
    "            \"stop\": stop_sequence\n",
    "        }\n",
    "        response = conversation(input_dict)\n",
    "\n",
    "        return response[\"response\"]\n",
    "\n",
    "\n",
    "    def add_history(self, name, message_content, channel_id) -> None:\n",
    "        # channel_id = str(message.channel.id)\n",
    "        # name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "        \n",
    "        # name = message.author.display_name\n",
    "        memory.add_input_only(f\"{name}: {message_content}\")\n",
    "        # dicts = messages_to_dict(self.memory.messages)\n",
    "        # self.history = '\\n'.join(message['data']['content'] for message in dicts)\n",
    "        print(f\"added to history: {name}: {message_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = Chatbot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=5100): Max retries exceeded with url: /api/modules (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000236F4C3E1D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\connection.py:200\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[0;32m    201\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[0;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[0;32m    203\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[0;32m    204\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[0;32m    205\u001b[0m     )\n\u001b[0;32m    206\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    497\u001b[0m         method,\n\u001b[0;32m    498\u001b[0m         url,\n\u001b[0;32m    499\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    500\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    501\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    502\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[0;32m    503\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[0;32m    504\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    507\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\connection.py:388\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 388\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders()\n\u001b[0;32m    390\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[0;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m \n\u001b[0;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[1;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\connection.py:236\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[0;32m    238\u001b[0m         \u001b[39m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\connection.py:215\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 215\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    216\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x00000236F4C3E1D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[1;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    846\u001b[0m )\n\u001b[0;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5100): Max retries exceeded with url: /api/modules (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000236F4C3E1D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m# Testing the summarize_text function\u001b[39;00m\n\u001b[0;32m     32\u001b[0m text_to_summarize \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mOn 18 June 2023, Titan, a submersible operated by American tourism and expeditions company OceanGate, imploded during an expedition to view the wreck of the Titanic in the North Atlantic Ocean off the coast of Newfoundland, Canada. On board the submersible were Stockton Rush, the CEO of OceanGate; Paul-Henri Nargeolet, a French deep sea explorer and Titanic expert; Hamish Harding, a British billionaire businessman; Shahzada Dawood, a Pakistani-British billionaire businessman; and Dawood\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms son Suleman.\u001b[39m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[39mCommunication with Titan was lost 1 hour and 45 minutes into its dive. Authorities were alerted when it failed to resurface at the scheduled time later that day. After the submersible had been missing for four days, a remotely operated underwater vehicle (ROV) discovered a debris field containing parts of Titan, about 500 metres (1,600 ft) from the bow of the Titanic. The search area was informed by the United States Navy\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms (USN) sonar detection of an acoustic signature consistent with an implosion around the time communications with the submersible ceased, suggesting the pressure hull had imploded while Titan was descending, resulting in the instantaneous deaths of all five occupants.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mMain article: Wreck of the Titanic\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[39mThe Titanic was a British ocean liner that sank in the North Atlantic Ocean on 15 April 1912, after colliding with an iceberg. More than 1,500 people died, making it the deadliest sinking of a single ship at the time.[7][8] In 1985, Robert Ballard located the wreck of the Titanic on the ocean floor, around 400 nautical miles (740 km; 460 mi) from the coast of Newfoundland.[9] The wreck lies at a depth of about 3,810 metres (12,500 feet; 2,080 fathoms).[10] Since its discovery, it has been a destination for research expeditions and tourism. By 2012, a century after its sinking, 140 people had visited the wreck site.[11]\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m---> 52\u001b[0m summary \u001b[39m=\u001b[39m summarize_text(text_to_summarize)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m summary:\n\u001b[0;32m     54\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSummary: \u001b[39m\u001b[39m{\u001b[39;00msummary\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36msummarize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msummarize_text\u001b[39m(text):\n\u001b[0;32m     18\u001b[0m     \u001b[39m# Checking if the summarize module is active\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m get_module_status():\n\u001b[0;32m     20\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSummarization module is not active.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mget_module_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_module_status\u001b[39m():\n\u001b[1;32m----> 5\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mhttp://localhost:5100/api/modules\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m      7\u001b[0m         modules \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmodules\u001b[39m\u001b[39m'\u001b[39m, [])\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\.venv\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5100): Max retries exceeded with url: /api/modules (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000236F4C3E1D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_module_status():\n",
    "    response = requests.get('http://localhost:5100/api/modules')\n",
    "    if response.status_code == 200:\n",
    "        modules = response.json().get('modules', [])\n",
    "        if 'summarize' in modules:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        print('Error: Could not connect to the API.')\n",
    "        return False\n",
    "\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Checking if the summarize module is active\n",
    "    if not get_module_status():\n",
    "        print('Summarization module is not active.')\n",
    "        return None\n",
    "\n",
    "    data = {'text': text}\n",
    "    response = requests.post('http://localhost:5100/api/summarize', json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('summary', '')\n",
    "    else:\n",
    "        print('Error: Could not summarize the text.')\n",
    "        return None\n",
    "\n",
    "# Testing the summarize_text function\n",
    "text_to_summarize = \"\"\"On 18 June 2023, Titan, a submersible operated by American tourism and expeditions company OceanGate, imploded during an expedition to view the wreck of the Titanic in the North Atlantic Ocean off the coast of Newfoundland, Canada. On board the submersible were Stockton Rush, the CEO of OceanGate; Paul-Henri Nargeolet, a French deep sea explorer and Titanic expert; Hamish Harding, a British billionaire businessman; Shahzada Dawood, a Pakistani-British billionaire businessman; and Dawood's son Suleman.\n",
    "\n",
    "Communication with Titan was lost 1 hour and 45 minutes into its dive. Authorities were alerted when it failed to resurface at the scheduled time later that day. After the submersible had been missing for four days, a remotely operated underwater vehicle (ROV) discovered a debris field containing parts of Titan, about 500 metres (1,600 ft) from the bow of the Titanic. The search area was informed by the United States Navy's (USN) sonar detection of an acoustic signature consistent with an implosion around the time communications with the submersible ceased, suggesting the pressure hull had imploded while Titan was descending, resulting in the instantaneous deaths of all five occupants.\n",
    "\n",
    "The search and rescue operation was conducted by an international team led by the United States Coast Guard (USCG), USN, and Canadian Coast Guard.[1] Support was provided by aircraft from the Royal Canadian Air Force and United States Air National Guard, a Royal Canadian Navy ship, as well as several commercial and research vessels and ROVs.[2][3]\n",
    "\n",
    "Numerous industry experts had raised concerns about the safety of the vessel. OceanGate executives, including Rush, had not sought certification for Titan, arguing that excessive safety protocols hindered innovation.[4]\n",
    "\n",
    "Background\n",
    "OceanGate\n",
    "Main article: OceanGate\n",
    "\n",
    "OceanGate CEO Stockton Rush, pictured in 2015\n",
    "OceanGate is a private company, founded in 2009 by Stockton Rush and Guillermo SÃ¶hnlein. Since 2010, it has transported paying customers in leased commercial submersibles off the coast of California, in the Gulf of Mexico, and in the Atlantic Ocean.[5] The company is based in Everett, Washington, U.S.[6]\n",
    "\n",
    "Rush realised that visiting shipwreck sites was a way to get media attention. OceanGate had previously conducted trips to other shipwrecks including its 2016 dive to the wreck of the Andrea Doria aboard their other submersible Cyclops 1. In 2019, Rush told Smithsonian magazine \"There's only one wreck that everyone knows ... If you ask people to name something underwater, it's going to be sharks, whales, Titanic\".[5]\n",
    "\n",
    "Titanic\n",
    "Main article: Wreck of the Titanic\n",
    "The Titanic was a British ocean liner that sank in the North Atlantic Ocean on 15 April 1912, after colliding with an iceberg. More than 1,500 people died, making it the deadliest sinking of a single ship at the time.[7][8] In 1985, Robert Ballard located the wreck of the Titanic on the ocean floor, around 400 nautical miles (740 km; 460 mi) from the coast of Newfoundland.[9] The wreck lies at a depth of about 3,810 metres (12,500 feet; 2,080 fathoms).[10] Since its discovery, it has been a destination for research expeditions and tourism. By 2012, a century after its sinking, 140 people had visited the wreck site.[11]\"\"\"\n",
    "summary = summarize_text(text_to_summarize)\n",
    "if summary:\n",
    "    print(f'Summary: {summary}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade langchain deeplake openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.1.0-cp310-cp310-win_amd64.whl (97 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2023.5.7 charset-normalizer-3.1.0 idna-3.4 requests-2.31.0 urllib3-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "# Please manually enter OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ACTIVELOOP_TOKEN\"] = getpass(\"Activeloop Token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = \"../../../..\"\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                docs.extend(loader.load_and_split())\n",
    "            except Exception as e:\n",
    "                pass\n",
    "print(f\"{len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "db = DeepLake.from_documents(\n",
    "    texts, embeddings, dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\"\n",
    ")\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\",\n",
    "    read_only=True,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    # filter based on source code\n",
    "    if \"something\" in x[\"text\"].data()[\"value\"]:\n",
    "        return False\n",
    "\n",
    "    # filter based on path e.g. extension\n",
    "    metadata = x[\"metadata\"].data()[\"value\"]\n",
    "    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n",
    "\n",
    "\n",
    "### turn on below for custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the class hierarchy?\",\n",
    "    # \"What classes are derived from the Chain class?\",\n",
    "    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n",
    "    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"Extract text from Substack blog post.\"\"\"\n",
    "    extra_info = {\n",
    "        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n",
    "        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n",
    "        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n",
    "    }\n",
    "    text = soup.select_one(\"div.available-content\").getText()\n",
    "    return text, extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, download_loader\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "index.query('What language is on this website?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, download_loader\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Website Index\",\n",
    "        func=lambda q: index.query(q),\n",
    "        description=f\"Useful when you want answer questions about the text on websites.\",\n",
    "    ),\n",
    "]\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=\"zero-shot-react-description\", memory=memory\n",
    ")\n",
    "\n",
    "output = agent_chain.run(input=\"What language is on this website?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load_data(urls=[\"https://langchain.readthedocs.io/en/latest/\"], custom_hostname=\"readthedocs.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "\n",
    "ImageVisionLLMReader = download_loader(\"ImageVisionLLMReader\")\n",
    "\n",
    "loader = ImageVisionLLMReader()\n",
    "documents = loader.load_data(file=Path('./cat.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count the amount of tokens in a string of text using tiktoken\n",
    "def count_tokens(text):\n",
    "    token_count = 0\n",
    "    for token in text.split():\n",
    "        token_count += 1\n",
    "    return token_count\n",
    "\n",
    "# function to count the amount of characters in a string of text using len()\n",
    "def count_characters(text):\n",
    "    character_count = len(text)\n",
    "    return character_count\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtiiuae/falcon-7b\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model)\n\u001b[1;32m----> 8\u001b[0m pipeline \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mpipeline(\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     11\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m     12\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16,\n\u001b[0;32m     13\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     14\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m     17\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mGirafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mDaniel: Hello, Girafatron!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mGirafatron:\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     eos_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:788\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    785\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    787\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 788\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    789\u001b[0m     model,\n\u001b[0;32m    790\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    791\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    792\u001b[0m     framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    793\u001b[0m     task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    794\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    795\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    796\u001b[0m )\n\u001b[0;32m    798\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    799\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     )\n\u001b[0;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    271\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:479\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m     model_class \u001b[39m=\u001b[39m get_class_from_dynamic_module(\n\u001b[0;32m    476\u001b[0m         class_ref, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    477\u001b[0m     )\n\u001b[0;32m    478\u001b[0m     _ \u001b[39m=\u001b[39m hub_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 479\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    480\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    481\u001b[0m     )\n\u001b[0;32m    482\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    483\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:2231\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2228\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2229\u001b[0m         )\n\u001b[0;32m   2230\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2231\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   2232\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2233\u001b[0m         )\n\u001b[0;32m   2235\u001b[0m \u001b[39mif\u001b[39;00m quantization_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2236\u001b[0m     quantization_config, kwargs \u001b[39m=\u001b[39m BitsAndBytesConfig\u001b[39m.\u001b[39mfrom_dict(\n\u001b[0;32m   2237\u001b[0m         config_dict\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mload_in_8bit\u001b[39m\u001b[39m\"\u001b[39m: load_in_8bit, \u001b[39m\"\u001b[39m\u001b[39mload_in_4bit\u001b[39m\u001b[39m\"\u001b[39m: load_in_4bit},\n\u001b[0;32m   2238\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2239\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2240\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Using cached einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from accelerate) (1.25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.6.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
